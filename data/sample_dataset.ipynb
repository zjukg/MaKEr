{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import defaultdict as ddict\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import dgl\n",
    "import torch\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_path = './NELL-995'\n",
    "data_path = './FB15K237'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dict_openke(dict_path):\n",
    "    \"\"\"\n",
    "    Read entity / relation dict.\n",
    "    Format: dict({id: entity / relation})\n",
    "    \"\"\"\n",
    "\n",
    "    element_dict = {}\n",
    "    with open(dict_path, 'r') as f:\n",
    "        f.readline()\n",
    "        for line in f:\n",
    "            element, id_ = line.strip().split('\\t')\n",
    "            element_dict[element] = int(id_)\n",
    "    return element_dict\n",
    "\n",
    "def read_data_openke(data_path):\n",
    "    \"\"\"\n",
    "    Read train / valid / test data.\n",
    "    \"\"\"\n",
    "    triples = []\n",
    "    with open(data_path, 'r') as f:\n",
    "        f.readline()\n",
    "        for line in f:\n",
    "            head, tail, relation = line.strip().split(' ')\n",
    "            h = int(head)\n",
    "            r = int(relation)\n",
    "            t = int(tail)\n",
    "            triples.append((h, r, t))\n",
    "    return triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_dict = read_dict_openke(os.path.join(data_path, 'entity2id.txt'))\n",
    "relation_dict = read_dict_openke(os.path.join(data_path, 'relation2id.txt'))\n",
    "\n",
    "entity_dict_inv = {v: k for k, v in entity_dict.items()}\n",
    "relation_dict_inv = {v: k for k, v in relation_dict.items()}\n",
    "\n",
    "train_triples = read_data_openke(os.path.join(data_path, 'train2id.txt'))\n",
    "valid_triples = read_data_openke(os.path.join(data_path, 'valid2id.txt'))\n",
    "test_triples = read_data_openke(os.path.join(data_path, 'test2id.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triples = train_triples + valid_triples + test_triples\n",
    "triples = torch.tensor(triples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sample test triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_undir = dgl.graph((torch.cat([triples[:, 0], triples[:, 2]]),\n",
    "                     torch.cat([triples[:, 2], triples[:, 0]])))\n",
    "\n",
    "g = dgl.graph((triples[:, 0], triples[:, 2]))\n",
    "g.edata['rel'] = triples[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_root_ent = 100\n",
    "rw_len = 10\n",
    "new_ratio = 0.1\n",
    "\n",
    "root_ent = np.random.choice(g_undir.num_nodes(), num_root_ent, replace=False)\n",
    "random_ent = torch.unique(dgl.sampling.random_walk(g_undir, root_ent, length=rw_len)[0])\n",
    "if -1 in random_ent:\n",
    "    random_ent = random_ent[1:]\n",
    "\n",
    "test_g = dgl.node_subgraph(g, random_ent)  # induce test triples from sampled entities\n",
    "\n",
    "test_ent = test_g.ndata[dgl.NID]  # entity in test triples\n",
    "test_rel = torch.unique(test_g.edata['rel'])  # relation in test triples\n",
    "\n",
    "test_new_ent = np.random.choice(test_ent, int(len(test_ent) * new_ratio), replace=False)  # entities that only appear in test triples \n",
    "test_new_rel = np.random.choice(test_rel, int(len(test_rel) * new_ratio), replace=False)  # relations that only appear in test triples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_remain_edge = np.setdiff1d(np.arange(g.num_edges()), test_g.edata[dgl.EID])\n",
    "test_remain_g = dgl.edge_subgraph(g, test_remain_edge)\n",
    "\n",
    "test_remain_tri = torch.stack([test_remain_g.ndata[dgl.NID][test_remain_g.edges()[0]],\n",
    "                               test_remain_g.edata['rel'], \n",
    "                               test_remain_g.ndata[dgl.NID][test_remain_g.edges()[1]]]).T.tolist()\n",
    "\n",
    "test_remain_tri_delnew = []\n",
    "for tri in tqdm(test_remain_tri):\n",
    "    h, r, t = tri\n",
    "    if h not in test_new_ent and t not in test_new_ent and r not in test_new_rel:\n",
    "        test_remain_tri_delnew.append(tri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sample valid triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triples_new = torch.tensor(test_remain_tri_delnew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_undir = dgl.graph((torch.cat([triples_new[:, 0], triples_new[:, 2]]),\n",
    "                     torch.cat([triples_new[:, 2], triples_new[:, 0]])))\n",
    "\n",
    "g = dgl.graph((triples_new[:, 0], triples_new[:, 2]))\n",
    "g.edata['rel'] = triples_new[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_ent = np.random.choice(g_undir.num_nodes(), num_root_ent, replace=False)\n",
    "random_ent = torch.unique(dgl.sampling.random_walk(g_undir, root_ent, length=rw_len)[0])\n",
    "if -1 in random_ent:\n",
    "    random_ent = random_ent[1:]\n",
    "\n",
    "valid_g = dgl.node_subgraph(g, random_ent)\n",
    "\n",
    "valid_ent = valid_g.ndata[dgl.NID]\n",
    "valid_rel = torch.unique(valid_g.edata['rel'])\n",
    "\n",
    "valid_new_ent = np.random.choice(valid_ent, int(len(valid_ent) * new_ratio), replace=False)\n",
    "valid_new_rel = np.random.choice(valid_rel, int(len(valid_rel) * new_ratio), replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_remain_edge = np.setdiff1d(np.arange(g.num_edges()), valid_g.edata[dgl.EID])\n",
    "valid_remain_g = dgl.edge_subgraph(g, valid_remain_edge)\n",
    "\n",
    "valid_remain_tri = torch.stack([valid_remain_g.ndata[dgl.NID][valid_remain_g.edges()[0]],\n",
    "                                valid_remain_g.edata['rel'], \n",
    "                                valid_remain_g.ndata[dgl.NID][valid_remain_g.edges()[1]]]).T.tolist()\n",
    "\n",
    "valid_remain_tri_delnew = []\n",
    "for tri in tqdm(valid_remain_tri):\n",
    "    h, r, t = tri\n",
    "    if h not in valid_new_ent and t not in valid_new_ent and r not in valid_new_rel:\n",
    "        valid_remain_tri_delnew.append(tri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sample train triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triples_new = torch.tensor(valid_remain_tri_delnew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_undir = dgl.graph((torch.cat([triples_new[:, 0], triples_new[:, 2]]),\n",
    "                     torch.cat([triples_new[:, 2], triples_new[:, 0]])))\n",
    "\n",
    "g = dgl.graph((triples_new[:, 0], triples_new[:, 2]))\n",
    "g.edata['rel'] = triples_new[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_root_ent = 100\n",
    "train_rw_len = 10\n",
    "\n",
    "root_ent = np.random.choice(g_undir.num_nodes(), num_train_root_ent, replace=False)\n",
    "random_ent = torch.unique(dgl.sampling.random_walk(g_undir, root_ent, length=train_rw_len)[0])\n",
    "if -1 in random_ent:\n",
    "    random_ent = random_ent[1:]\n",
    "\n",
    "train_g = dgl.node_subgraph(g, random_ent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## re-index triples in train/valid/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_triples = torch.stack([train_g.ndata[dgl.NID][train_g.edges()[0]],\n",
    "                               train_g.edata['rel'], \n",
    "                               train_g.ndata[dgl.NID][train_g.edges()[1]]]).T.tolist()\n",
    "\n",
    "test_triples = torch.stack([test_g.ndata[dgl.NID][test_g.edges()[0]],\n",
    "                               test_g.edata['rel'], \n",
    "                               test_g.ndata[dgl.NID][test_g.edges()[1]]]).T.tolist()\n",
    "\n",
    "valid_triples = torch.stack([valid_g.ndata[dgl.NID][valid_g.edges()[0]],\n",
    "                               valid_g.edata['rel'], \n",
    "                               valid_g.ndata[dgl.NID][valid_g.edges()[1]]]).T.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### re-index train triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reidx_train(triples):\n",
    "    ent_reidx = dict()\n",
    "    rel_reidx = dict()\n",
    "    \n",
    "    entidx = 0\n",
    "    relidx = 0\n",
    "    \n",
    "    reidx_triples = []\n",
    "    for tri in triples:\n",
    "        h, r, t = tri\n",
    "        if h not in ent_reidx.keys():\n",
    "            ent_reidx[h] = entidx\n",
    "            entidx += 1\n",
    "        if t not in ent_reidx.keys():\n",
    "            ent_reidx[t] = entidx\n",
    "            entidx += 1\n",
    "        if r not in rel_reidx.keys():\n",
    "            rel_reidx[r] = relidx\n",
    "            relidx += 1\n",
    "    \n",
    "        reidx_triples.append((ent_reidx[h], rel_reidx[r], ent_reidx[t]))\n",
    "    \n",
    "    return reidx_triples, ent_reidx, rel_reidx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_triples, train_ent_reidx, train_rel_reidx = reidx_train(train_triples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ent2id = {entity_dict_inv[k]: v for k, v in train_ent_reidx.items()}\n",
    "train_rel2id = {relation_dict_inv[k]: v for k, v in train_rel_reidx.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### re-index valid/test triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reidx_eval(triples, train_ent_reidx, train_rel_reidx):\n",
    "    ent_reidx = dict()\n",
    "    rel_reidx = dict()\n",
    "    \n",
    "    entidx = 0\n",
    "    relidx = 0\n",
    "    \n",
    "    ent_freq = ddict(int)\n",
    "    rel_freq = ddict(int)\n",
    "    \n",
    "    reidx_triples = []\n",
    "    for tri in triples:\n",
    "        h, r, t = tri\n",
    "        if h not in ent_reidx.keys():\n",
    "            ent_reidx[h] = entidx\n",
    "            entidx += 1\n",
    "        if t not in ent_reidx.keys():\n",
    "            ent_reidx[t] = entidx\n",
    "            entidx += 1\n",
    "        if r not in rel_reidx.keys():\n",
    "            rel_reidx[r] = relidx\n",
    "            relidx += 1\n",
    "            \n",
    "        ent_freq[ent_reidx[h]] += 1\n",
    "        ent_freq[ent_reidx[t]] += 1\n",
    "        rel_freq[rel_reidx[r]] += 1\n",
    "    \n",
    "        reidx_triples.append((ent_reidx[h], rel_reidx[r], ent_reidx[t]))\n",
    "    \n",
    "    ent_reidx_inv = {v: k for k, v in ent_reidx.items()}\n",
    "    rel_reidx_inv = {v: k for k, v in rel_reidx.items()}\n",
    "    \n",
    "    ent_map_list = [train_ent_reidx[ent_reidx_inv[i]] if ent_reidx_inv[i] in train_ent_reidx.keys() else -1\n",
    "                      for i in range(len(ent_reidx))]\n",
    "    rel_map_list = [train_rel_reidx[rel_reidx_inv[i]] if rel_reidx_inv[i] in train_rel_reidx.keys() else -1\n",
    "                      for i in range(len(rel_reidx))]\n",
    "    \n",
    "    return reidx_triples, ent_freq, rel_freq, ent_reidx, rel_reidx, ent_map_list, rel_map_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_triples, valid_ent_freq, valid_rel_freq, valid_ent_reidx, valid_rel_reidx, \\\n",
    "    valid_ent_map_list, valid_rel_map_list = reidx_eval(valid_triples, train_ent_reidx, train_rel_reidx)\n",
    "\n",
    "test_triples, test_ent_freq, test_rel_freq, test_ent_reidx, test_rel_reidx, \\\n",
    "    test_ent_map_list, test_rel_map_list = reidx_eval(test_triples, train_ent_reidx, train_rel_reidx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_ent2id = {entity_dict_inv[k]: v for k, v in valid_ent_reidx.items()}\n",
    "valid_rel2id = {relation_dict_inv[k]: v for k, v in valid_rel_reidx.items()}\n",
    "\n",
    "test_ent2id = {entity_dict_inv[k]: v for k, v in test_ent_reidx.items()}\n",
    "test_rel2id = {relation_dict_inv[k]: v for k, v in test_rel_reidx.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## split triples in valid/test into support and query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_triples(triples, ent_freq, rel_freq, ent_map_list, rel_map_list):\n",
    "    ent_freq = copy.deepcopy(ent_freq)\n",
    "    rel_freq = copy.deepcopy(rel_freq)\n",
    "    \n",
    "    support_triples = []\n",
    "    query_triples = []\n",
    "    \n",
    "    query_uent = []\n",
    "    query_urel = []\n",
    "    query_uboth = []\n",
    "\n",
    "    random.shuffle(triples)\n",
    "    for idx, tri in enumerate(triples):\n",
    "        h, r, t = tri\n",
    "        test_flag = (ent_map_list[h] == -1 or ent_map_list[t] == -1 or rel_map_list[r] == -1)\n",
    "        \n",
    "        if (ent_freq[h] > 2 and ent_freq[t] > 2 and rel_freq[r] > 2) and test_flag:\n",
    "            append_flag = False\n",
    "            if ent_map_list[h] != -1 and ent_map_list[t] != -1 and rel_map_list[r] == -1:\n",
    "                if len(query_urel) <= int(len(triples) * 0.1):\n",
    "                    query_urel.append(tri)\n",
    "                    append_flag = True\n",
    "            elif (ent_map_list[h] == -1 or ent_map_list[t] == -1) and rel_map_list[r] != -1:\n",
    "                if len(query_uent) <= int(len(triples) * 0.1):\n",
    "                    query_uent.append(tri)\n",
    "                    append_flag = True\n",
    "            else:\n",
    "                if len(query_uboth) <= int(len(triples) * 0.1):\n",
    "                    query_uboth.append(tri)\n",
    "                    append_flag = True\n",
    "            \n",
    "            if append_flag:\n",
    "                ent_freq[h] -= 1\n",
    "                ent_freq[t] -= 1\n",
    "                rel_freq[r] -= 1\n",
    "            else:\n",
    "                support_triples.append(tri)\n",
    "        else:\n",
    "            support_triples.append(tri)\n",
    "    \n",
    "    return support_triples, query_uent, query_urel, query_uboth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_sup_tris, valid_que_uent, valid_que_urel, valid_que_uboth = split_triples(valid_triples, \n",
    "                                                                                valid_ent_freq, valid_rel_freq, \n",
    "                                                                                valid_ent_map_list, valid_rel_map_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sup_tris, test_que_uent, test_que_urel, test_que_uboth = split_triples(test_triples, \n",
    "                                                                            test_ent_freq, test_rel_freq,\n",
    "                                                                            test_ent_map_list, test_rel_map_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = {'train': {'triples': train_triples, 'ent2id': train_ent2id, 'rel2id': train_rel2id},\n",
    "             'valid': {'support': valid_sup_tris, 'query': valid_que_uent + valid_que_urel + valid_que_uboth, \n",
    "                       'ent_map_list': valid_ent_map_list, 'rel_map_list': valid_rel_map_list,\n",
    "                       'ent2id': valid_ent2id, 'rel2id': valid_rel2id},\n",
    "             'test': {'support': test_sup_tris, 'query_uent': test_que_uent,\n",
    "                      'query_urel': test_que_urel, 'query_uboth': test_que_uboth,\n",
    "                      'ent_map_list': test_ent_map_list, 'rel_map_list': test_rel_map_list,\n",
    "                      'ent2id': test_ent2id, 'rel2id': test_rel2id}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(data_dict, open('./test_data.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data statistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_data = pickle.load(open('./test_data.pkl', 'rb'))\n",
    "\n",
    "valid_num_new_ent = np.sum(np.array(load_data['valid']['ent_map_list']) == -1)\n",
    "valid_num_new_rel = np.sum(np.array(load_data['valid']['rel_map_list']) == -1)\n",
    "test_num_new_ent = np.sum(np.array(load_data['test']['ent_map_list']) == -1)\n",
    "test_num_new_rel = np.sum(np.array(load_data['test']['rel_map_list']) == -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('train:')\n",
    "print(f\"num_ent: {len(load_data['train']['ent2id'])}\")\n",
    "print(f\"num_rel: {len(load_data['train']['rel2id'])}\")\n",
    "print(f\"num_tri: {len(load_data['train']['triples'])}\")\n",
    "\n",
    "print('valid:')\n",
    "print(f\"num_ent: {len(load_data['valid']['ent2id'])}(new: {valid_num_new_ent}, {valid_num_new_ent/len(load_data['valid']['ent2id']):.2})\")\n",
    "print(f\"num_rel: {len(load_data['valid']['rel2id'])}(new: {valid_num_new_rel}, {valid_num_new_rel/len(load_data['valid']['rel2id']):.2})\")\n",
    "print(f\"num_sup: {len(load_data['valid']['support'])}\")\n",
    "print(f\"num_que: {len(load_data['valid']['query'])}\")\n",
    "\n",
    "print('test:')\n",
    "print(f\"num_ent: {len(load_data['test']['ent2id'])}(new: {test_num_new_ent}, {test_num_new_ent/len(load_data['test']['ent2id']):.2})\")\n",
    "print(f\"num_rel: {len(load_data['test']['rel2id'])}(new: {test_num_new_rel}, {test_num_new_rel/len(load_data['test']['rel2id']):.2})\")\n",
    "print(f\"num_sup: {len(load_data['test']['support'])}\")\n",
    "print(f\"num_que_uent: {len(load_data['test']['query_uent'])}\")\n",
    "print(f\"num_que_urel: {len(load_data['test']['query_urel'])}\")\n",
    "print(f\"num_que_uboth: {len(load_data['test']['query_uboth'])}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch18bu",
   "language": "python",
   "name": "torch18bu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
